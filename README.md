# Abstrat
Human brain possesses an extraordinary ability to attend to a specific sound source in a crowded, noisy environment such as a cocktail party. Auditory attention detection (AAD) aims to automatically identify this neural activity from brain signals, such as electroencephalography (EEG). Given the dynamic and nonlinear nature of EEG signals, we propose a spiking long short-term memory (LSTM) network to capture the temporal features of EEG data. Additionally, we introduce a spiking temporal attention mechanism that dynamically assigns differentiated weights, thereby enhancing the representation of EEG features. We evaluate our proposed spiking temporal LSTM model, named ST-LSTM, on a widely used AAD dataset through comprehensive experiments. Results demonstrate that ST-LSTM outperforms other competing models, especially in low-latency settings. Moreover, its low power consumption makes it a practical option for integration into intelligent hearing aids, including neuro-steered hearing aids, and other brain-computer interfaces. Therefore, the ST-LSTM holds significant potential to improve the accuracy and efficiency of AAD systems, particularly in real-world applications.
![image](https://github.com/SCUT-IEL/ST-LSTM/blob/main/Model.png)
